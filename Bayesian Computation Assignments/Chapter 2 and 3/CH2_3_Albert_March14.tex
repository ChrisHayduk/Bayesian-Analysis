%\documentclass[12pt,addpoints]{exam}   % Print w/o solutions
\documentclass[12pt,addpoints,answers]{exam}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}   % Print solutions
%%%% comment out ONE of the above lines  
%    - the first line prints the document without the solutions, just questions
%    - the second prints the document with solutions.
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{lscape}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{multicol}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\singlespacing
%\onehalfspacing
%\doublespacing


\title{Assignment Covering Chapters 2 \& 3 of \emph{Bayesian Computation with R}}

\author{Chris Hayduk}
\date{\today}

\maketitle



\begin{questions}
\question \textbf{Chapter 2: Introduction to Bayesian Thinking} Write your own code for the following:
%%%%%%%%%
\begin{parts}
\part Simulating a sample from a posterior distribution when you have a histogram prior.  
%%%%%%%%%
\part Prior predictive density function, \texttt{pdiscp()}.  
%%%%%%%%%
\part Prior predictive density function, \texttt{pbetap()}.
%%%%%%%%%
\part Discrete credible interval, \texttt{discint}.
\end{parts}
%%%%%%%%%
\begin{solution}
\begin{parts}
\part Let's start by defining a function to generate the values for the histogram prior:
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#Function to generate prior values for each value in x}
\hlstd{get_histprior_value} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{,} \hlkwc{histprior}\hlstd{)\{}
  \hlstd{vec} \hlkwb{<-} \hlkwd{rep}\hlstd{(}\hlnum{NA}\hlstd{,} \hlkwc{length}\hlstd{=}\hlkwd{length}\hlstd{(x))}
  \hlkwa{for}\hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{length}\hlstd{(vec))\{}
    \hlstd{new_vec} \hlkwb{<-} \hlstd{histprior[histprior[,}\hlnum{1}\hlstd{]}\hlopt{>=}\hlstd{x[i],]}

    \hlcom{#Check if new_vec is matrix or vector and index accordingly}
    \hlkwa{if}\hlstd{(}\hlopt{!}\hlkwd{is.null}\hlstd{(}\hlkwd{ncol}\hlstd{(new_vec)))\{}
      \hlstd{vec[i]} \hlkwb{<-} \hlstd{new_vec[}\hlnum{1}\hlstd{,}\hlnum{2}\hlstd{]}
    \hlstd{\}} \hlkwa{else}\hlstd{\{}
      \hlstd{vec[i]} \hlkwb{<-} \hlstd{new_vec[}\hlnum{2}\hlstd{]}
    \hlstd{\}}
  \hlstd{\}}

  \hlkwd{return}\hlstd{(vec)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}
\newpage
Now let's prepare the data for input into our new \texttt{get\_histprior\_value} function:
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#Generate points for interval}
\hlstd{interval} \hlkwb{<-} \hlkwd{seq}\hlstd{(}\hlnum{0.1}\hlstd{,} \hlnum{1}\hlstd{,} \hlkwc{by} \hlstd{=} \hlnum{0.1}\hlstd{)}

\hlcom{#Prior probability}
\hlstd{prior} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{5.2}\hlstd{,} \hlnum{8}\hlstd{,} \hlnum{7.2}\hlstd{,} \hlnum{4.6}\hlstd{,} \hlnum{2.1}\hlstd{,} \hlnum{0.7}\hlstd{,} \hlnum{0.1}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{0}\hlstd{)}
\hlstd{prior} \hlkwb{<-} \hlstd{prior}\hlopt{/}\hlkwd{sum}\hlstd{(prior)}

\hlcom{#Create the histogram prior}
\hlstd{histprior} \hlkwb{<-} \hlkwd{sample}\hlstd{(interval,} \hlnum{10000}\hlstd{,} \hlkwc{replace}\hlstd{=}\hlnum{TRUE}\hlstd{,} \hlkwc{prob} \hlstd{= prior)}
\hlstd{histprior} \hlkwb{<-} \hlkwd{table}\hlstd{(histprior)}\hlopt{/}\hlkwd{sum}\hlstd{(}\hlkwd{table}\hlstd{(histprior))}
\hlstd{histprior} \hlkwb{<-} \hlkwd{as.matrix}\hlstd{(histprior)}
\hlstd{names} \hlkwb{<-} \hlkwd{rownames}\hlstd{(histprior)}
\hlkwd{rownames}\hlstd{(histprior)} \hlkwb{<-} \hlkwa{NULL}
\hlstd{histprior} \hlkwb{<-} \hlkwd{cbind}\hlstd{(}\hlkwd{as.numeric}\hlstd{(names),}\hlkwd{as.numeric}\hlstd{(histprior))}
\hlstd{histprior} \hlkwb{<-} \hlkwd{rbind}\hlstd{(histprior,} \hlkwd{c}\hlstd{(}\hlnum{0.9}\hlstd{,} \hlnum{0}\hlstd{))}
\hlstd{histprior} \hlkwb{<-} \hlkwd{rbind}\hlstd{(histprior,} \hlkwd{c}\hlstd{(}\hlnum{1.0}\hlstd{,} \hlnum{0}\hlstd{))}
\end{alltt}
\end{kframe}
\end{knitrout}
\newpage
Now let's plot the prior:
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#Output histogram of prior}
\hlkwd{curve}\hlstd{(}\hlkwd{get_histprior_value}\hlstd{(x, histprior),} \hlkwc{from}\hlstd{=}\hlnum{0}\hlstd{,} \hlkwc{to} \hlstd{=} \hlnum{1}\hlstd{,}
      \hlkwc{ylim} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{0.3}\hlstd{),} \hlkwc{n} \hlstd{=} \hlnum{10000}\hlstd{,}
      \hlkwc{xlab}\hlstd{=}\hlstr{"p"}\hlstd{,} \hlkwc{ylab} \hlstd{=} \hlstr{"Prior density"}\hlstd{,} \hlkwc{main} \hlstd{=} \hlstr{"Histogram Prior"}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-4-1} 

\end{knitrout}
\newpage
Finally, let's generate and plot the posterior:
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{s} \hlkwb{<-} \hlnum{11}
\hlstd{f} \hlkwb{<-} \hlnum{16}

\hlstd{p} \hlkwb{<-} \hlkwd{seq}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{1}\hlstd{,} \hlkwc{length} \hlstd{=} \hlnum{10000}\hlstd{)}

\hlstd{post} \hlkwb{<-} \hlkwd{get_histprior_value}\hlstd{(p, histprior)} \hlopt{*} \hlkwd{dbeta}\hlstd{(p, s}\hlopt{+}\hlnum{1}\hlstd{, f}\hlopt{+}\hlnum{1}\hlstd{)}

\hlstd{post} \hlkwb{<-} \hlstd{post}\hlopt{/}\hlkwd{sum}\hlstd{(post)}

\hlstd{ps} \hlkwb{<-} \hlkwd{sample}\hlstd{(p,} \hlkwc{replace} \hlstd{=} \hlnum{TRUE}\hlstd{,} \hlkwc{prob} \hlstd{= post)}

\hlkwd{hist}\hlstd{(ps,} \hlkwc{xlab}\hlstd{=}\hlstr{"p"}\hlstd{,} \hlkwc{main}\hlstd{=}\hlstr{"Simulated Draws from the Posterior Distribution of p"}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-5-1} 

\end{knitrout}
\newpage
\part Let's start with \texttt{pdiscp()}:
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{pdiscp} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{p}\hlstd{,} \hlkwc{prior}\hlstd{,} \hlkwc{m}\hlstd{,} \hlkwc{ys}\hlstd{)\{}
  \hlstd{pred} \hlkwb{<-} \hlkwd{rep}\hlstd{(}\hlnum{NA}\hlstd{,} \hlkwc{length} \hlstd{=} \hlkwd{length}\hlstd{(ys))}

  \hlcom{#Loop through possible y values}
  \hlkwa{for}\hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{length}\hlstd{(ys))\{}
    \hlstd{val} \hlkwb{<-} \hlnum{0}
    \hlcom{#Loop through possible proportions in discrete prior}
    \hlkwa{for}\hlstd{(j} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{length}\hlstd{(p))\{}
      \hlstd{f} \hlkwb{<-} \hlkwd{choose}\hlstd{(m, ys[i])} \hlopt{*} \hlstd{(p[j])}\hlopt{^}\hlstd{(ys[i])} \hlopt{*} \hlstd{(}\hlnum{1}\hlopt{-}\hlstd{p[j])}\hlopt{^}\hlstd{(m}\hlopt{-}\hlstd{ys[i])}
      \hlstd{g} \hlkwb{<-} \hlstd{prior[j]}
      \hlstd{val} \hlkwb{<-} \hlstd{val} \hlopt{+} \hlstd{(f}\hlopt{*}\hlstd{g)}
    \hlstd{\}}
    \hlstd{pred[i]} \hlkwb{<-} \hlstd{val}
  \hlstd{\}}

  \hlcom{#Return vector of probabilities for each y value}
  \hlkwd{return}\hlstd{(pred)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#Test run}
\hlstd{p} \hlkwb{<-} \hlkwd{seq}\hlstd{(}\hlnum{0.05}\hlstd{,} \hlnum{0.95}\hlstd{,} \hlkwc{by}\hlstd{=}\hlnum{0.1}\hlstd{)}
\hlstd{prior} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{5.2}\hlstd{,} \hlnum{8}\hlstd{,} \hlnum{7.2}\hlstd{,} \hlnum{4.6}\hlstd{,} \hlnum{2.1}\hlstd{,} \hlnum{0.7}\hlstd{,} \hlnum{0.1}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{0}\hlstd{)}
\hlstd{prior} \hlkwb{<-} \hlstd{prior}\hlopt{/}\hlkwd{sum}\hlstd{(prior)}
\hlstd{m} \hlkwb{<-} \hlnum{20}
\hlstd{ys} \hlkwb{<-} \hlnum{0}\hlopt{:}\hlnum{20}
\hlstd{pred} \hlkwb{<-} \hlkwd{pdiscp}\hlstd{(p, prior, m, ys)}

\hlkwd{round}\hlstd{(}\hlkwd{cbind}\hlstd{(}\hlnum{0}\hlopt{:}\hlnum{20}\hlstd{, pred),} \hlnum{3}\hlstd{)}
\end{alltt}
\begin{verbatim}
##           pred
##  [1,]  0 0.020
##  [2,]  1 0.044
##  [3,]  2 0.069
##  [4,]  3 0.092
##  [5,]  4 0.106
##  [6,]  5 0.112
##  [7,]  6 0.110
##  [8,]  7 0.102
##  [9,]  8 0.089
## [10,]  9 0.074
## [11,] 10 0.059
## [12,] 11 0.044
## [13,] 12 0.031
## [14,] 13 0.021
## [15,] 14 0.013
## [16,] 15 0.007
## [17,] 16 0.004
## [18,] 17 0.002
## [19,] 18 0.001
## [20,] 19 0.000
## [21,] 20 0.000
\end{verbatim}
\end{kframe}
\end{knitrout}
\newpage
\part Now let's try \texttt{pbetap()}:
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{pbetap} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{ab}\hlstd{,} \hlkwc{m}\hlstd{,} \hlkwc{ys}\hlstd{)\{}
  \hlstd{pred} \hlkwb{<-} \hlkwd{rep}\hlstd{(}\hlnum{NA}\hlstd{,} \hlkwc{length} \hlstd{=} \hlkwd{length}\hlstd{(ys))}

  \hlkwa{for}\hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{length}\hlstd{(ys))\{}
    \hlstd{choose} \hlkwb{<-} \hlkwd{choose}\hlstd{(m, ys[i])}

    \hlstd{beta_num} \hlkwb{<-} \hlkwd{beta}\hlstd{(ab[}\hlnum{1}\hlstd{]} \hlopt{+} \hlstd{ys[i], ab[}\hlnum{2}\hlstd{]} \hlopt{+} \hlstd{m} \hlopt{-} \hlstd{ys[i])}

    \hlstd{beta_denom} \hlkwb{<-} \hlkwd{beta}\hlstd{(ab[}\hlnum{1}\hlstd{], ab[}\hlnum{2}\hlstd{])}

    \hlstd{pred[i]} \hlkwb{<-} \hlstd{choose}\hlopt{*}\hlstd{(beta_num}\hlopt{/}\hlstd{beta_denom)}
  \hlstd{\}}

  \hlcom{#Return vector of probabilities for each y value}
  \hlkwd{return}\hlstd{(pred)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#Test run}
\hlstd{ab} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{3.26}\hlstd{,} \hlnum{7.19}\hlstd{)}
\hlstd{m} \hlkwb{<-} \hlnum{20}
\hlstd{ys} \hlkwb{<-} \hlnum{0}\hlopt{:}\hlnum{20}
\hlstd{pred} \hlkwb{<-} \hlkwd{pbetap}\hlstd{(ab, m, ys)}
\hlkwd{round}\hlstd{(}\hlkwd{cbind}\hlstd{(}\hlnum{0}\hlopt{:}\hlnum{20}\hlstd{, pred),} \hlnum{3}\hlstd{)}
\end{alltt}
\begin{verbatim}
##           pred
##  [1,]  0 0.018
##  [2,]  1 0.045
##  [3,]  2 0.072
##  [4,]  3 0.095
##  [5,]  4 0.108
##  [6,]  5 0.114
##  [7,]  6 0.111
##  [8,]  7 0.102
##  [9,]  8 0.088
## [10,]  9 0.073
## [11,] 10 0.057
## [12,] 11 0.043
## [13,] 12 0.030
## [14,] 13 0.020
## [15,] 14 0.012
## [16,] 15 0.007
## [17,] 16 0.004
## [18,] 17 0.002
## [19,] 18 0.001
## [20,] 19 0.000
## [21,] 20 0.000
\end{verbatim}
\end{kframe}
\end{knitrout}
\newpage
\part Finally, here's \texttt{discint}:
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{discint} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{dist}\hlstd{,} \hlkwc{covprob}\hlstd{)\{}
  \hlstd{total_prob} \hlkwb{<-} \hlnum{0}
  \hlstd{y_list} \hlkwb{<-} \hlkwd{c}\hlstd{()}

  \hlcom{#Find max probability value}
  \hlcom{#Centers credible interval around y value with max probability}
  \hlstd{i} \hlkwb{<-} \hlkwd{which.is.max}\hlstd{(dist[,}\hlnum{2}\hlstd{])}
  \hlstd{total_prob} \hlkwb{<-} \hlstd{total_prob} \hlopt{+} \hlstd{dist[i,}\hlnum{2}\hlstd{]}
  \hlstd{y_list} \hlkwb{<-} \hlkwd{c}\hlstd{(y_list, dist[i,}\hlnum{1}\hlstd{])}

  \hlstd{i_prev} \hlkwb{<-} \hlstd{i}\hlopt{-}\hlnum{1}
  \hlstd{i_next} \hlkwb{<-} \hlstd{i}\hlopt{+}\hlnum{1}

  \hlkwa{while}\hlstd{(total_prob} \hlopt{<} \hlstd{covprob)\{}
    \hlcom{#Check if previous row is within range of matrix}
    \hlkwa{if}\hlstd{(i_prev} \hlopt{>=} \hlnum{1} \hlopt{&} \hlstd{i_prev} \hlopt{<=} \hlkwd{nrow}\hlstd{(dist))\{}
      \hlstd{prev_val} \hlkwb{<-} \hlstd{dist[i_prev,}\hlnum{2}\hlstd{]}
    \hlstd{\}} \hlkwa{else}\hlstd{\{}
      \hlstd{prev_val} \hlkwb{<-} \hlopt{-}\hlnum{1}
    \hlstd{\}}

    \hlcom{#Check if next row is within range of matrix}
    \hlkwa{if}\hlstd{(i_next} \hlopt{>=} \hlnum{1} \hlopt{&} \hlstd{i_next} \hlopt{<=} \hlkwd{nrow}\hlstd{(dist))\{}
      \hlstd{next_val} \hlkwb{<-} \hlstd{dist[i_next,}\hlnum{2}\hlstd{]}
    \hlstd{\}} \hlkwa{else}\hlstd{\{}
      \hlstd{next_val} \hlkwb{<-} \hlopt{-}\hlnum{1}
    \hlstd{\}}

    \hlcom{#Compare values for i_next and i_prev}
    \hlkwa{if}\hlstd{(dist[i_next,}\hlnum{2}\hlstd{]} \hlopt{>=} \hlstd{dist[i_prev,}\hlnum{2}\hlstd{])\{}
      \hlstd{total_prob} \hlkwb{<-} \hlstd{total_prob} \hlopt{+} \hlstd{dist[i_next,} \hlnum{2}\hlstd{]}
      \hlstd{y_list} \hlkwb{<-} \hlkwd{c}\hlstd{(y_list, dist[i_next,} \hlnum{1}\hlstd{])}
      \hlstd{i_next} \hlkwb{<-} \hlstd{i_next} \hlopt{+} \hlnum{1}
    \hlstd{\}} \hlkwa{else}\hlstd{\{}
      \hlstd{total_prob} \hlkwb{<-} \hlstd{total_prob} \hlopt{+} \hlstd{dist[i_prev,} \hlnum{2}\hlstd{]}
      \hlstd{y_list} \hlkwb{<-} \hlkwd{c}\hlstd{(y_list, dist[i_prev,} \hlnum{1}\hlstd{])}
      \hlstd{i_prev} \hlkwb{<-} \hlstd{i_prev} \hlopt{-} \hlnum{1}
    \hlstd{\}}
  \hlstd{\}}

  \hlstd{output} \hlkwb{<-} \hlkwd{list}\hlstd{(}\hlkwc{prob} \hlstd{= total_prob,} \hlkwc{set} \hlstd{=} \hlkwd{sort}\hlstd{(y_list))}

  \hlkwd{return}\hlstd{(output)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#Test run}
\hlstd{p} \hlkwb{<-} \hlkwd{rbeta}\hlstd{(}\hlnum{1000}\hlstd{,} \hlnum{3.26}\hlstd{,} \hlnum{7.19}\hlstd{)}
\hlstd{y} \hlkwb{<-} \hlkwd{rbinom}\hlstd{(}\hlnum{1000}\hlstd{,} \hlnum{20}\hlstd{, p)}
\hlstd{freq} \hlkwb{<-} \hlkwd{table}\hlstd{(y)}
\hlstd{ys} \hlkwb{<-} \hlkwd{as.integer}\hlstd{(}\hlkwd{names}\hlstd{(freq))}
\hlstd{predprob} \hlkwb{<-} \hlstd{freq}\hlopt{/}\hlkwd{sum}\hlstd{(freq)}

\hlstd{dist} \hlkwb{<-} \hlkwd{cbind}\hlstd{(ys, predprob)}
\hlstd{covprob} \hlkwb{<-} \hlnum{0.9}

\hlkwd{discint}\hlstd{(dist, covprob)}
\end{alltt}
\begin{verbatim}
## $prob
## [1] 0.911
## 
## $set
##  [1]  1  2  3  4  5  6  7  8  9 10 11
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{parts}
\end{solution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\question \textbf{Chapter 2: Introduction to Bayesian Thinking}  Show how the author derived the predictive density formula below with a $beta(a,b)$ prior (pg. 31 of book):
\begin{eqnarray*}
f(\tilde{y}) &=& \int f_B(\tilde{y}|m,p)g(p)dp \\
             &=& {m\choose\tilde{y}}\frac{B(a+\tilde{y},b+m-\tilde{y})}{B(a,b)}
\end{eqnarray*}
where $\tilde{y}=0,\ldots,m$.
%%%%%%%%%
\begin{solution}
\begin{align*}
f(\tilde{y}) &= \int f_B(\tilde{y}|m,p)g(p)dp \\
&= \int_{0}^{1} {m\choose \tilde{y}}p^{\tilde{y}}(1-p)^{m-\tilde{y}}\left(\frac{1}{B(a,b)} p^{a-1}(1-p)^{b-1}\right) dp\\
&= {m\choose \tilde{y}} \frac{1}{B(a,b)} \int_{0}^{1} p^{\tilde{y}}(1-p)^{m-\tilde{y}}\left(p^{a-1}(1-p)^{b-1}\right) dp\\
&= {m\choose \tilde{y}} \frac{1}{B(a,b)} \int_{0}^{1} p^{\tilde{y}+a-1}(1-p)^{m-\tilde{y}+b-1} dp\\
&= {m\choose \tilde{y}} \frac{1}{B(a,b)} B(a+\tilde{y}, b+m-\tilde{y})\\
&= {m\choose \tilde{y}} \frac{B(a+\tilde{y}, b+m-\tilde{y})}{B(a,b)}
\end{align*}
\end{solution}
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\question \textbf{Chapter 2: Introduction to Bayesian Thinking} Exercise 5.
%%%%%%%%%
\begin{solution}
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#a)}
\hlstd{mu} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{20}\hlstd{,} \hlnum{30}\hlstd{,} \hlnum{40}\hlstd{,} \hlnum{50}\hlstd{,} \hlnum{60}\hlstd{,} \hlnum{70}\hlstd{)}
\hlstd{prior} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{.1}\hlstd{,} \hlnum{.15}\hlstd{,} \hlnum{.25}\hlstd{,} \hlnum{.25}\hlstd{,} \hlnum{.15}\hlstd{,} \hlnum{.1}\hlstd{)}
\hlstd{prior} \hlkwb{<-} \hlstd{prior}\hlopt{/}\hlkwd{sum}\hlstd{(prior)}

\hlcom{#b)}
\hlstd{y} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{38.6}\hlstd{,} \hlnum{42.4}\hlstd{,} \hlnum{57.5}\hlstd{,} \hlnum{40.5}\hlstd{,} \hlnum{51.7}\hlstd{,} \hlnum{67.1}\hlstd{,} \hlnum{33.4}\hlstd{,} \hlnum{60.9}\hlstd{,} \hlnum{64.1}\hlstd{,} \hlnum{40.1}\hlstd{,} \hlnum{40.7}\hlstd{,} \hlnum{6.4}\hlstd{)}
\hlstd{ybar} \hlkwb{<-} \hlkwd{mean}\hlstd{(y)}

\hlcom{#c)}
\hlstd{n} \hlkwb{<-} \hlkwd{length}\hlstd{(y)}
\hlstd{sigma_squared} \hlkwb{<-} \hlkwd{var}\hlstd{(y)}

\hlstd{like} \hlkwb{<-} \hlkwd{exp}\hlstd{(}\hlopt{-}\hlstd{(n}\hlopt{/}\hlstd{(}\hlnum{2}\hlopt{*}\hlstd{sigma_squared))}\hlopt{*}\hlstd{(mu} \hlopt{-} \hlstd{ybar)}\hlopt{^}\hlnum{2}\hlstd{)}

\hlcom{#d)}
\hlstd{post} \hlkwb{<-} \hlstd{prior}\hlopt{*}\hlstd{like}\hlopt{/}\hlkwd{sum}\hlstd{(prior}\hlopt{*}\hlstd{like)}

\hlcom{#e)}
\hlstd{dist} \hlkwb{<-} \hlkwd{cbind}\hlstd{(mu, post)}
\hlkwd{discint}\hlstd{(dist,} \hlnum{0.8}\hlstd{)}
\end{alltt}
\begin{verbatim}
## $prob
##      post 
## 0.9921244 
## 
## $set
## mu mu 
## 40 50
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{solution}
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\question \textbf{Chapter 2: Introduction to Bayesian Thinking} Exercise 6.
%%%%%%%%%
\begin{solution}
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#a)}
\hlstd{lambda} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{0.5}\hlstd{,} \hlnum{1}\hlstd{,} \hlnum{1.5}\hlstd{,} \hlnum{2}\hlstd{,} \hlnum{2.5}\hlstd{,} \hlnum{3}\hlstd{)}
\hlstd{prior} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{0.1}\hlstd{,} \hlnum{0.2}\hlstd{,} \hlnum{0.3}\hlstd{,} \hlnum{0.2}\hlstd{,} \hlnum{0.15}\hlstd{,} \hlnum{0.05}\hlstd{)}
\hlstd{prior} \hlkwb{<-} \hlstd{prior}\hlopt{/}\hlkwd{sum}\hlstd{(prior)}

\hlstd{pred} \hlkwb{<-} \hlkwd{rep}\hlstd{(}\hlnum{NA}\hlstd{,} \hlkwc{length} \hlstd{=} \hlkwd{length}\hlstd{(lambda))}
\hlstd{y} \hlkwb{<-} \hlnum{12}

\hlkwa{for}\hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{length}\hlstd{(lambda))\{}
  \hlstd{pred[i]} \hlkwb{<-} \hlstd{prior[i]}\hlopt{*}\hlkwd{exp}\hlstd{(}\hlopt{-}\hlnum{6}\hlopt{*}\hlstd{lambda[i])}\hlopt{*}\hlstd{(}\hlnum{6}\hlopt{*}\hlstd{lambda[i])}\hlopt{^}\hlnum{12}
\hlstd{\}}
\hlstd{pred} \hlkwb{<-} \hlstd{pred}\hlopt{/}\hlkwd{sum}\hlstd{(pred)}
\hlstd{pred} \hlkwb{<-} \hlkwd{cbind}\hlstd{(lambda, pred)}
\hlkwd{print}\hlstd{(}\hlkwd{round}\hlstd{(pred,}\hlnum{5}\hlstd{))}
\end{alltt}
\begin{verbatim}
##      lambda    pred
## [1,]    0.5 0.00009
## [2,]    1.0 0.03679
## [3,]    1.5 0.35652
## [4,]    2.0 0.37357
## [5,]    2.5 0.20299
## [6,]    3.0 0.03004
\end{verbatim}
\begin{alltt}
\hlcom{#b)}
\hlstd{predictive_prob} \hlkwb{<-} \hlkwd{rep}\hlstd{(}\hlnum{NA}\hlstd{,} \hlkwc{length} \hlstd{=} \hlkwd{length}\hlstd{(lambda))}
\hlkwa{for}\hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{length}\hlstd{(lambda))\{}
  \hlstd{predictive_prob[i]} \hlkwb{<-} \hlkwd{exp}\hlstd{(}\hlopt{-}\hlnum{7}\hlopt{*}\hlstd{lambda[i])}\hlopt{*}\hlstd{pred[i]}
\hlstd{\}}

\hlkwd{sum}\hlstd{(predictive_prob)}
\end{alltt}
\begin{verbatim}
## [1] 0.01605361
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{solution}
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\question  \textbf{Chapter 3: Single-Parameter Models} Write your own versions of the functions \texttt{binomial.beta.mix()} and \texttt{pbetat()}.
%%%%%%%%%
\begin{solution}
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{binomial.beta.mix} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{probs}\hlstd{,} \hlkwc{betpar}\hlstd{,} \hlkwc{data}\hlstd{)\{}
  \hlstd{m} \hlkwb{=} \hlstd{data[}\hlnum{1}\hlstd{]} \hlopt{+} \hlstd{data[}\hlnum{2}\hlstd{]}

  \hlstd{lambda_data} \hlkwb{<-} \hlstd{(probs[}\hlnum{1}\hlstd{]} \hlopt{*} \hlkwd{pbetap}\hlstd{(betpar[}\hlnum{1}\hlstd{,], m, data[}\hlnum{1}\hlstd{]))}\hlopt{/}
    \hlstd{(probs[}\hlnum{1}\hlstd{]} \hlopt{*} \hlkwd{pbetap}\hlstd{(betpar[}\hlnum{1}\hlstd{,], m, data[}\hlnum{1}\hlstd{])} \hlopt{+}
       \hlstd{probs[}\hlnum{2}\hlstd{]} \hlopt{*} \hlkwd{pbetap}\hlstd{(betpar[}\hlnum{2}\hlstd{,], m, data[}\hlnum{1}\hlstd{]))}

  \hlstd{posterior} \hlkwb{<-} \hlkwd{c}\hlstd{()}
  \hlstd{posterior}\hlopt{$}\hlstd{probs} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlkwd{c}\hlstd{(lambda_data, (}\hlnum{1}\hlopt{-}\hlstd{lambda_data)),} \hlkwc{nrow}\hlstd{=}\hlnum{1}\hlstd{,} \hlkwc{ncol}\hlstd{=}\hlnum{2}\hlstd{)}
  \hlkwd{colnames}\hlstd{(posterior}\hlopt{$}\hlstd{probs)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"beta.par1"}\hlstd{,} \hlstr{"beta.par2"}\hlstd{)}

  \hlstd{posterior}\hlopt{$}\hlstd{betapar} \hlkwb{<-} \hlkwd{rbind}\hlstd{(}\hlkwd{c}\hlstd{(betapar[}\hlnum{1}\hlstd{,}\hlnum{1}\hlstd{]} \hlopt{+} \hlstd{data[}\hlnum{1}\hlstd{], betapar[}\hlnum{1}\hlstd{,}\hlnum{2}\hlstd{]} \hlopt{+} \hlstd{data[}\hlnum{2}\hlstd{]),}
                             \hlkwd{c}\hlstd{(betapar[}\hlnum{2}\hlstd{,}\hlnum{1}\hlstd{]} \hlopt{+} \hlstd{data[}\hlnum{1}\hlstd{], betapar[}\hlnum{2}\hlstd{,}\hlnum{2}\hlstd{]} \hlopt{+} \hlstd{data[}\hlnum{2}\hlstd{]))}
  \hlkwd{rownames}\hlstd{(posterior}\hlopt{$}\hlstd{betapar)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"beta.par1"}\hlstd{,} \hlstr{"beta.par2"}\hlstd{)}
  \hlkwd{colnames}\hlstd{(posterior}\hlopt{$}\hlstd{betapar)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{""}\hlstd{,} \hlstr{""}\hlstd{)}

  \hlkwd{return}\hlstd{(posterior)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

Here is an example run using the values from the book (p. 51-52):
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#Example run}
\hlstd{probs} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{.5}\hlstd{,} \hlnum{.5}\hlstd{)}
\hlstd{beta.par1} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{6}\hlstd{,} \hlnum{14}\hlstd{)}
\hlstd{beta.par2} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{14}\hlstd{,} \hlnum{6}\hlstd{)}
\hlstd{betapar} \hlkwb{<-} \hlkwd{rbind}\hlstd{(beta.par1, beta.par2)}
\hlstd{data} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{7}\hlstd{,}\hlnum{3}\hlstd{)}

\hlstd{post} \hlkwb{<-} \hlkwd{binomial.beta.mix}\hlstd{(probs, betapar, data)}

\hlstd{post}\hlopt{$}\hlstd{probs}
\end{alltt}
\begin{verbatim}
##       beta.par1 beta.par2
## [1,] 0.09269663 0.9073034
\end{verbatim}
\begin{alltt}
\hlstd{post}\hlopt{$}\hlstd{betapar}
\end{alltt}
\begin{verbatim}
##                
## beta.par1 13 17
## beta.par2 21  9
\end{verbatim}
\end{kframe}
\end{knitrout}
\newpage
Now let's plot the posterior and the prior:
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{curve}\hlstd{(post}\hlopt{$}\hlstd{probs[}\hlnum{1}\hlstd{]}\hlopt{*}\hlkwd{dbeta}\hlstd{(x,}\hlnum{13}\hlstd{,}\hlnum{17}\hlstd{)} \hlopt{+} \hlstd{post}\hlopt{$}\hlstd{probs[}\hlnum{2}\hlstd{]}\hlopt{*}\hlkwd{dbeta}\hlstd{(x,}\hlnum{21}\hlstd{,}\hlnum{9}\hlstd{),}
      \hlkwc{from} \hlstd{=} \hlnum{0}\hlstd{,} \hlkwc{to} \hlstd{=} \hlnum{1}\hlstd{,} \hlkwc{lwd}\hlstd{=}\hlnum{3}\hlstd{,} \hlkwc{xlab} \hlstd{=} \hlstr{"P"}\hlstd{,} \hlkwc{ylab} \hlstd{=} \hlstr{"Density"}\hlstd{)}
\hlkwd{curve}\hlstd{(}\hlnum{0.5}\hlopt{*}\hlkwd{dbeta}\hlstd{(x,}\hlnum{6}\hlstd{,}\hlnum{12}\hlstd{)}\hlopt{+}\hlnum{0.5}\hlopt{*}\hlkwd{dbeta}\hlstd{(x,}\hlnum{12}\hlstd{,}\hlnum{6}\hlstd{),}\hlnum{0}\hlstd{,}\hlnum{1}\hlstd{,}\hlkwc{add}\hlstd{=}\hlnum{TRUE}\hlstd{)}
\hlkwd{legend}\hlstd{(}\hlstr{"topleft"}\hlstd{,} \hlkwc{legend}\hlstd{=}\hlkwd{c}\hlstd{(}\hlstr{"Prior"}\hlstd{,} \hlstr{"Posterior"}\hlstd{),} \hlkwc{lwd}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,}\hlnum{3}\hlstd{))}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-16-1} 

\end{knitrout}
\end{solution}
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\question \textbf{Chapter 3: Single-Parameter Models} On pg. 55, Albert writes, ``One can show that the posterior probability that the coin is fair is given by:''
\begin{eqnarray*}
\lambda(y) &=& \frac{0.5P_0(Y\leq 5)}{0.5P_0(Y\leq 5) + 0.5P_1(Y\leq 5)}.
\end{eqnarray*}
Show this.
%%%%%%%%%
\begin{solution}
On pg. 54, Albert shows that the posterior density of the model where the coin is fair ($p = 0.5$) is given by:
$$ \lambda(y) = \frac{.5p(y|.5)}{.5p(y|.5) + .5m_1(y)} $$
where $p(y|.5)$ is the binomial density for $y$ when $p = .5$ and $m_1(y)$ is the (prior) predictive density for $y$ using the beta density.

The equation on pg. 55 essentially takes the above equation and sums over the probability of $y = 0, 1, 2, 3, 4, 5$.
\end{solution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\question \textbf{Chapter 3: Single-Parameter Models} Do all Exercises at the end of the chapter.  
\end{questions}
%%%%%%%%%
\begin{solution}
\begin{enumerate}
\item
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#a)}
\hlstd{data} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{10}\hlstd{,} \hlnum{9}\hlstd{,} \hlnum{8}\hlstd{,} \hlnum{11}\hlstd{,} \hlnum{3}\hlstd{,} \hlnum{3}\hlstd{,} \hlnum{8}\hlstd{,} \hlnum{8}\hlstd{,} \hlnum{11}\hlstd{)}

\hlstd{theta} \hlkwb{<-} \hlkwd{seq}\hlstd{(}\hlkwc{from} \hlstd{=} \hlopt{-}\hlnum{2}\hlstd{,} \hlkwc{to} \hlstd{=} \hlnum{12}\hlstd{,} \hlkwc{by} \hlstd{=} \hlnum{0.1}\hlstd{)}

\hlcom{#b)}
\hlstd{posterior_density} \hlkwb{<-} \hlkwd{rep}\hlstd{(}\hlnum{1}\hlstd{,} \hlkwd{length}\hlstd{(theta))}

\hlkwa{for}\hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{length}\hlstd{(theta))\{}
  \hlkwa{for}\hlstd{(j} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{length}\hlstd{(data))\{}
    \hlstd{posterior_density[i]} \hlkwb{<-} \hlstd{posterior_density[i]} \hlopt{*} \hlstd{(}\hlnum{1}\hlopt{/}\hlstd{(}\hlnum{1}\hlopt{+}\hlstd{((data[j]} \hlopt{-} \hlstd{theta[i])}\hlopt{^}\hlnum{2}\hlstd{)))}
  \hlstd{\}}
\hlstd{\}}

\hlstd{posterior_density} \hlkwb{<-} \hlstd{posterior_density}\hlopt{/}\hlkwd{sum}\hlstd{(posterior_density)}

\hlcom{#c)}
\hlstd{df} \hlkwb{<-} \hlkwd{data.frame}\hlstd{(}\hlkwc{theta} \hlstd{= theta,} \hlkwc{posterior} \hlstd{= posterior_density)}

\hlkwd{ggplot}\hlstd{(df,} \hlkwd{aes}\hlstd{(}\hlkwc{x} \hlstd{= theta,} \hlkwc{y} \hlstd{= posterior))} \hlopt{+}
  \hlkwd{geom_line}\hlstd{()} \hlopt{+}
  \hlkwd{xlab}\hlstd{(}\hlkwd{TeX}\hlstd{(}\hlstr{'$\textbackslash{}\textbackslash{}theta$'}\hlstd{))} \hlopt{+}
  \hlkwd{ggtitle}\hlstd{(}\hlkwd{TeX}\hlstd{(}\hlstr{'Posterior Density of $\textbackslash{}\textbackslash{}theta$'}\hlstd{))} \hlopt{+}
  \hlstd{theme.info}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-17-1} 
\begin{kframe}\begin{alltt}
\hlcom{#d)}
\hlstd{samples} \hlkwb{<-} \hlkwd{sample}\hlstd{(theta,} \hlnum{100000}\hlstd{,} \hlkwc{replace} \hlstd{=} \hlnum{TRUE}\hlstd{,} \hlkwc{prob} \hlstd{= posterior_density)}

\hlcom{#Posterior mean}
\hlkwd{mean}\hlstd{(samples)}
\end{alltt}
\begin{verbatim}
## [1] 8.629352
\end{verbatim}
\begin{alltt}
\hlcom{#Posterior standard deviation}
\hlkwd{sd}\hlstd{(samples)}
\end{alltt}
\begin{verbatim}
## [1] 0.6469175
\end{verbatim}
\end{kframe}
\end{knitrout}

\item Let $\theta = 1/\lambda = \lambda^{-1}$. Then we have $\lambda = \theta^{-1}$ and:
\begin{align*}
\lambda^{-n-1}e^{-s/\lambda} &= (\theta^{-1})^{-n-1}e^{-s/(\theta^{-1})}\\
&= \theta^{-1(-n-1)}e^{-s\theta}\\
&= \theta^{n+1}e^{-s\theta}
\end{align*}

So, $\theta$ has a gamma density with shape parameter $n + 2$ and rate parameters $s$.
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#b)}
\hlstd{burn_times} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{751}\hlstd{,} \hlnum{594}\hlstd{,} \hlnum{1213}\hlstd{,} \hlnum{1126}\hlstd{,} \hlnum{819}\hlstd{)}

\hlstd{n} \hlkwb{<-} \hlkwd{length}\hlstd{(burn_times)}
\hlstd{s} \hlkwb{<-} \hlkwd{sum}\hlstd{(burn_times)}

\hlstd{theta_draws} \hlkwb{<-} \hlkwd{rgamma}\hlstd{(}\hlnum{1000}\hlstd{,} \hlkwc{shape} \hlstd{= n}\hlopt{+}\hlnum{2}\hlstd{,} \hlkwc{rate} \hlstd{= s)}

\hlcom{#c)}
\hlstd{lambda_draws} \hlkwb{<-} \hlnum{1}\hlopt{/}\hlstd{theta_draws}

\hlcom{#d)}
\hlstd{greater_than_1000} \hlkwb{<-} \hlkwd{sum}\hlstd{(lambda_draws} \hlopt{>} \hlnum{1000}\hlstd{)}

\hlstd{posterior_prob_greater_than_1000} \hlkwb{<-} \hlstd{greater_than_1000}\hlopt{/}\hlkwd{length}\hlstd{(lambda_draws)}

\hlkwd{print}\hlstd{(posterior_prob_greater_than_1000)}
\end{alltt}
\begin{verbatim}
## [1] 0.156
\end{verbatim}
\end{kframe}
\end{knitrout}
\newpage
\item We know $100 \leq N \leq 200$. So,

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{N} \hlkwb{<-} \hlkwd{seq}\hlstd{(}\hlkwc{from} \hlstd{=} \hlnum{100}\hlstd{,} \hlkwc{to} \hlstd{=} \hlnum{200}\hlstd{,} \hlkwc{by} \hlstd{=} \hlnum{1}\hlstd{)}
\hlstd{post} \hlkwb{<-} \hlkwd{rep}\hlstd{(}\hlnum{0}\hlstd{,} \hlkwc{length} \hlstd{=} \hlkwd{length}\hlstd{(N))}

\hlstd{n} \hlkwb{<-} \hlkwd{length}\hlstd{(y)}

\hlkwa{for}\hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{length}\hlstd{(post))\{}
  \hlstd{post[i]} \hlkwb{<-} \hlnum{1}\hlopt{/}\hlstd{(N[i]}\hlopt{^}\hlstd{n)}
\hlstd{\}}

\hlstd{samples} \hlkwb{<-} \hlkwd{sample}\hlstd{(N,} \hlnum{100000}\hlstd{,} \hlkwc{replace} \hlstd{=} \hlnum{TRUE}\hlstd{,} \hlkwc{prob} \hlstd{= post)}

\hlcom{#Posterior mean}
\hlkwd{mean}\hlstd{(samples)}
\end{alltt}
\begin{verbatim}
## [1] 144.2164
\end{verbatim}
\begin{alltt}
\hlcom{#Posterior standard deviation}
\hlkwd{sd}\hlstd{(samples)}
\end{alltt}
\begin{verbatim}
## [1] 29.00468
\end{verbatim}
\end{kframe}
\end{knitrout}
\newpage
\item 
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#a)}
\hlstd{p1} \hlkwb{<-} \hlkwd{rbeta}\hlstd{(}\hlnum{1000}\hlstd{,} \hlkwc{shape1} \hlstd{=} \hlnum{100}\hlstd{,} \hlkwc{shape2} \hlstd{=} \hlnum{100}\hlstd{)}

\hlstd{df1} \hlkwb{<-} \hlkwd{data.frame}\hlstd{(}\hlkwc{p1} \hlstd{= p1)}

\hlkwd{ggplot}\hlstd{(}\hlkwc{data} \hlstd{= df1,} \hlkwd{aes}\hlstd{(}\hlkwc{x}\hlstd{=p1))} \hlopt{+} \hlkwd{geom_histogram}\hlstd{(}\hlkwc{bins} \hlstd{=} \hlnum{15}\hlstd{)} \hlopt{+} \hlstd{theme.info}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-20-1} 

\end{knitrout}

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#Mean of P1. Target is 0.5}
\hlkwd{mean}\hlstd{(p1)}
\end{alltt}
\begin{verbatim}
## [1] 0.5007184
\end{verbatim}
\begin{alltt}
\hlcom{#Probability that 0.44 < p < 0.56. Target is a probability of 0.9}
\hlstd{(}\hlkwd{sum}\hlstd{(p1} \hlopt{<} \hlnum{0.56}\hlstd{)} \hlopt{-} \hlkwd{sum}\hlstd{(p1} \hlopt{<} \hlnum{0.44}\hlstd{))}\hlopt{/}\hlkwd{length}\hlstd{(p1)}
\end{alltt}
\begin{verbatim}
## [1] 0.91
\end{verbatim}
\begin{alltt}
\hlstd{p2} \hlkwb{<-} \hlnum{0.9}\hlopt{*}\hlkwd{rbeta}\hlstd{(}\hlnum{1000}\hlstd{,} \hlnum{500}\hlstd{,} \hlnum{500}\hlstd{)} \hlopt{+} \hlnum{0.1}\hlopt{*}\hlkwd{rbeta}\hlstd{(}\hlnum{1000}\hlstd{,} \hlnum{1}\hlstd{,} \hlnum{1}\hlstd{)}

\hlstd{df2} \hlkwb{<-} \hlkwd{data.frame}\hlstd{(}\hlkwc{p2} \hlstd{= p2)}

\hlkwd{ggplot}\hlstd{(}\hlkwc{data} \hlstd{= df2,} \hlkwd{aes}\hlstd{(}\hlkwc{x}\hlstd{=p2))} \hlopt{+} \hlkwd{geom_histogram}\hlstd{(}\hlkwc{bins} \hlstd{=} \hlnum{15}\hlstd{)} \hlopt{+} \hlstd{theme.info}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-21-1} 

\end{knitrout}

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#Mean of P2. Target is 0.5}
\hlkwd{mean}\hlstd{(p2)}
\end{alltt}
\begin{verbatim}
## [1] 0.4997092
\end{verbatim}
\begin{alltt}
\hlcom{#Probability that 0.44 < p < 0.56. Target is a probability of 0.9}
\hlstd{(}\hlkwd{sum}\hlstd{(p2} \hlopt{<} \hlnum{0.56}\hlstd{)} \hlopt{-} \hlkwd{sum}\hlstd{(p2} \hlopt{<} \hlnum{0.44}\hlstd{))}\hlopt{/}\hlkwd{length}\hlstd{(p2)}
\end{alltt}
\begin{verbatim}
## [1] 0.965
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#b)}
\hlstd{s} \hlkwb{<-} \hlnum{45}
\hlstd{f} \hlkwb{<-} \hlnum{55}

\hlstd{a1} \hlkwb{<-} \hlnum{100}
\hlstd{b1} \hlkwb{<-} \hlnum{100}

\hlstd{posterior1} \hlkwb{<-} \hlkwd{rbeta}\hlstd{(}\hlnum{1000}\hlstd{,a1}\hlopt{+}\hlstd{s, b1}\hlopt{+}\hlstd{f)}

\hlkwd{mean}\hlstd{(posterior1)}
\end{alltt}
\begin{verbatim}
## [1] 0.4829703
\end{verbatim}
\begin{alltt}
\hlkwd{quantile}\hlstd{(posterior1,} \hlkwd{c}\hlstd{(}\hlnum{0.05}\hlstd{,} \hlnum{0.95}\hlstd{))}
\end{alltt}
\begin{verbatim}
##        5%       95% 
## 0.4359717 0.5294894
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{probs} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{.9}\hlstd{,} \hlnum{.1}\hlstd{)}
\hlstd{beta.par1} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{500}\hlstd{,} \hlnum{500}\hlstd{)}
\hlstd{beta.par2} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{1}\hlstd{)}
\hlstd{betapar} \hlkwb{<-} \hlkwd{rbind}\hlstd{(beta.par1, beta.par2)}
\hlstd{data} \hlkwb{<-} \hlkwd{c}\hlstd{(s,f)}

\hlstd{post} \hlkwb{<-} \hlkwd{binomial.beta.mix}\hlstd{(probs, betapar, data)}

\hlstd{posterior2} \hlkwb{<-} \hlstd{post}\hlopt{$}\hlstd{probs[}\hlnum{1}\hlstd{]} \hlopt{*}
  \hlkwd{rbeta}\hlstd{(}\hlnum{1000}\hlstd{, post}\hlopt{$}\hlstd{betapar[}\hlnum{1}\hlstd{,}\hlnum{1}\hlstd{]} \hlopt{+}
          \hlstd{s, post}\hlopt{$}\hlstd{betapar[}\hlnum{1}\hlstd{,}\hlnum{2}\hlstd{]} \hlopt{+} \hlstd{f)} \hlopt{+}
  \hlstd{post}\hlopt{$}\hlstd{probs[}\hlnum{2}\hlstd{]} \hlopt{*}
  \hlkwd{rbeta}\hlstd{(}\hlnum{1000}\hlstd{, post}\hlopt{$}\hlstd{betapar[}\hlnum{2}\hlstd{,}\hlnum{1}\hlstd{]} \hlopt{+}
          \hlstd{s, post}\hlopt{$}\hlstd{betapar[}\hlnum{2}\hlstd{,}\hlnum{2}\hlstd{]} \hlopt{+} \hlstd{f)}

\hlkwd{mean}\hlstd{(posterior2)}
\end{alltt}
\begin{verbatim}
## [1] 0.4499606
\end{verbatim}
\begin{alltt}
\hlkwd{quantile}\hlstd{(posterior2,} \hlkwd{c}\hlstd{(}\hlnum{0.05}\hlstd{,} \hlnum{0.95}\hlstd{))}
\end{alltt}
\begin{verbatim}
##        5%       95% 
## 0.3944848 0.5083229
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#c)}
\hlstd{s} \hlkwb{<-} \hlnum{30}
\hlstd{f} \hlkwb{<-} \hlnum{70}

\hlstd{s} \hlkwb{<-} \hlnum{45}
\hlstd{f} \hlkwb{<-} \hlnum{55}

\hlstd{a1} \hlkwb{<-} \hlnum{100}
\hlstd{b1} \hlkwb{<-} \hlnum{100}

\hlstd{posterior1} \hlkwb{<-} \hlkwd{rbeta}\hlstd{(}\hlnum{1000}\hlstd{,a1}\hlopt{+}\hlstd{s, b1}\hlopt{+}\hlstd{f)}

\hlkwd{mean}\hlstd{(posterior1)}
\end{alltt}
\begin{verbatim}
## [1] 0.4853572
\end{verbatim}
\begin{alltt}
\hlkwd{quantile}\hlstd{(posterior1,} \hlkwd{c}\hlstd{(}\hlnum{0.05}\hlstd{,} \hlnum{0.95}\hlstd{))}
\end{alltt}
\begin{verbatim}
##        5%       95% 
## 0.4375929 0.5313869
\end{verbatim}
\begin{alltt}
\hlstd{probs} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{.9}\hlstd{,} \hlnum{.1}\hlstd{)}
\hlstd{beta.par1} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{500}\hlstd{,} \hlnum{500}\hlstd{)}
\hlstd{beta.par2} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{1}\hlstd{)}
\hlstd{betapar} \hlkwb{<-} \hlkwd{rbind}\hlstd{(beta.par1, beta.par2)}
\hlstd{data} \hlkwb{<-} \hlkwd{c}\hlstd{(s,f)}

\hlstd{post} \hlkwb{<-} \hlkwd{binomial.beta.mix}\hlstd{(probs, betapar, data)}

\hlstd{posterior2} \hlkwb{<-} \hlstd{post}\hlopt{$}\hlstd{probs[}\hlnum{1}\hlstd{]} \hlopt{*} \hlkwd{rbeta}\hlstd{(}\hlnum{1000}\hlstd{, post}\hlopt{$}\hlstd{betapar[}\hlnum{1}\hlstd{,}\hlnum{1}\hlstd{]} \hlopt{+}
                                      \hlstd{s, post}\hlopt{$}\hlstd{betapar[}\hlnum{1}\hlstd{,}\hlnum{2}\hlstd{]} \hlopt{+} \hlstd{f)} \hlopt{+}
  \hlstd{post}\hlopt{$}\hlstd{probs[}\hlnum{2}\hlstd{]} \hlopt{*}
  \hlkwd{rbeta}\hlstd{(}\hlnum{1000}\hlstd{, post}\hlopt{$}\hlstd{betapar[}\hlnum{2}\hlstd{,}\hlnum{1}\hlstd{]} \hlopt{+} \hlstd{s, post}\hlopt{$}\hlstd{betapar[}\hlnum{2}\hlstd{,}\hlnum{2}\hlstd{]} \hlopt{+} \hlstd{f)}

\hlkwd{mean}\hlstd{(posterior2)}
\end{alltt}
\begin{verbatim}
## [1] 0.450403
\end{verbatim}
\begin{alltt}
\hlkwd{quantile}\hlstd{(posterior2,} \hlkwd{c}\hlstd{(}\hlnum{0.05}\hlstd{,} \hlnum{0.95}\hlstd{))}
\end{alltt}
\begin{verbatim}
##        5%       95% 
## 0.3927499 0.5086557
\end{verbatim}
\end{kframe}
\end{knitrout}

In both cases, the means of the posteriors and the 90\% confidence intervals are affected by the choice of prior. The difference is not overly drastic, but the means may differ by up to 0.04 depending upon the simulation.

\item 
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#a)}
\hlstd{p_value} \hlkwb{<-} \hlnum{2} \hlopt{*} \hlstd{(}\hlnum{1}\hlopt{-}\hlkwd{pbinom}\hlstd{(}\hlnum{8}\hlstd{,} \hlnum{20}\hlstd{,} \hlnum{0.2}\hlstd{))}

\hlkwd{print}\hlstd{(p_value)}
\end{alltt}
\begin{verbatim}
## [1] 0.01996357
\end{verbatim}
\end{kframe}
\end{knitrout}

This value is smaller than 0.05, so we can reject the hypothesis that the person does not have ESP.

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{pbetat} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{p}\hlstd{,} \hlkwc{prob}\hlstd{,} \hlkwc{beta_params}\hlstd{,} \hlkwc{data}\hlstd{)\{}
  \hlstd{y} \hlkwb{<-} \hlstd{data[}\hlnum{1}\hlstd{]}
  \hlstd{f} \hlkwb{<-} \hlstd{data[}\hlnum{2}\hlstd{]}
  \hlstd{a} \hlkwb{<-} \hlstd{beta_params[}\hlnum{1}\hlstd{]}
  \hlstd{b} \hlkwb{<-} \hlstd{beta_params[}\hlnum{2}\hlstd{]}

  \hlstd{m1} \hlkwb{<-} \hlkwd{dbinom}\hlstd{(y, n, p)} \hlopt{*} \hlkwd{dbeta}\hlstd{(p, a,b)}\hlopt{/}\hlkwd{dbeta}\hlstd{(p, a}\hlopt{+}\hlstd{y, b}\hlopt{+}\hlstd{n}\hlopt{-}\hlstd{y)}

  \hlcom{#Lambda is the posterior probability that p = 0.2}
  \hlstd{lambda} \hlkwb{<-} \hlkwd{dbinom}\hlstd{(y,n,p)}\hlopt{/}\hlstd{(}\hlkwd{dbinom}\hlstd{(y,n,p)} \hlopt{+} \hlstd{m1)}

  \hlkwd{return}\hlstd{(lambda)}
\hlstd{\}}

\hlcom{#b)}
\hlstd{n} \hlkwb{<-} \hlnum{20}
\hlstd{y} \hlkwb{<-} \hlnum{8}
\hlstd{a} \hlkwb{<-} \hlnum{1}
\hlstd{b} \hlkwb{<-} \hlnum{4}
\hlstd{p} \hlkwb{<-} \hlnum{0.2}
\hlstd{prob} \hlkwb{<-} \hlnum{0.5}

\hlcom{#Lambda is the posterior probability that p = 0.2}
\hlstd{lambda} \hlkwb{<-} \hlkwd{pbetat}\hlstd{(p, prob,} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{4}\hlstd{),} \hlkwd{c}\hlstd{(y, n}\hlopt{-}\hlstd{y))}

\hlkwd{print}\hlstd{(lambda)}
\end{alltt}
\begin{verbatim}
## [1] 0.3410395
\end{verbatim}
\end{kframe}
\end{knitrout}

There is more evidence for the hypothesis that $p = 0.2$ than is suggested by the p-value above

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#c)}
\hlcom{#Prior 1}
\hlkwd{print}\hlstd{(}\hlkwd{pbetat}\hlstd{(p, prob,} \hlkwd{c}\hlstd{(}\hlnum{.5}\hlstd{,} \hlnum{2}\hlstd{),} \hlkwd{c}\hlstd{(y, n}\hlopt{-}\hlstd{y)))}
\end{alltt}
\begin{verbatim}
## [1] 0.3900752
\end{verbatim}
\begin{alltt}
\hlcom{#Prior 2}
\hlkwd{print}\hlstd{(}\hlkwd{pbetat}\hlstd{(p, prob,} \hlkwd{c}\hlstd{(}\hlnum{2}\hlstd{,} \hlnum{8}\hlstd{),} \hlkwd{c}\hlstd{(y, n}\hlopt{-}\hlstd{y)))}
\end{alltt}
\begin{verbatim}
## [1] 0.328591
\end{verbatim}
\begin{alltt}
\hlcom{#Prior 3}
\hlkwd{print}\hlstd{(}\hlkwd{pbetat}\hlstd{(p, prob,} \hlkwd{c}\hlstd{(}\hlnum{8}\hlstd{,} \hlnum{32}\hlstd{),} \hlkwd{c}\hlstd{(y, n}\hlopt{-}\hlstd{y)))}
\end{alltt}
\begin{verbatim}
## [1] 0.3855337
\end{verbatim}
\end{kframe}
\end{knitrout}

All four Bayesian computations assert that the posterior probability that $p = 2$ is between about $0.32$ and $0.4$. The range of posterior values is quite large and suggests a fairly sizeable probability that $p = 2$ is true. Since actual ESP is highly unlikely, we should not conclude that this person has ESP, which contradicts the analysis we performed using p-values.
\newpage
\item
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#a)}
\hlstd{mu} \hlkwb{<-} \hlkwd{seq}\hlstd{(}\hlkwc{from} \hlstd{=} \hlnum{0}\hlstd{,} \hlkwc{to} \hlstd{=} \hlnum{140}\hlstd{,} \hlkwc{by} \hlstd{=} \hlnum{1}\hlstd{)}
\hlstd{sigma} \hlkwb{<-} \hlnum{10}
\hlstd{s} \hlkwb{<-} \hlnum{1}
\hlstd{f} \hlkwb{<-} \hlnum{17}

\hlstd{likelihood} \hlkwb{<-} \hlstd{(}\hlkwd{pnorm}\hlstd{(}\hlnum{70}\hlstd{, mu, sigma))}\hlopt{^}\hlstd{s} \hlopt{*} \hlstd{(}\hlnum{1} \hlopt{-} \hlkwd{pnorm}\hlstd{(}\hlnum{70}\hlstd{, mu, sigma))}\hlopt{^}\hlstd{f}

\hlstd{likelihood} \hlkwb{<-} \hlstd{likelihood}\hlopt{/}\hlkwd{sum}\hlstd{(likelihood)}

\hlstd{data} \hlkwb{<-} \hlkwd{data.frame}\hlstd{(}\hlkwc{mu} \hlstd{= mu,} \hlkwc{likelihood} \hlstd{= likelihood)}

\hlkwd{ggplot}\hlstd{(data,} \hlkwd{aes}\hlstd{(}\hlkwc{x}\hlstd{= mu,} \hlkwc{y}\hlstd{= likelihood))} \hlopt{+}
  \hlkwd{geom_line}\hlstd{()} \hlopt{+}
  \hlkwd{ggtitle}\hlstd{(}\hlkwd{TeX}\hlstd{(}\hlstr{"Posterior Density of $\textbackslash{}\textbackslash{}mu$"}\hlstd{))} \hlopt{+}
  \hlkwd{ylab}\hlstd{(}\hlstr{"Posterior"}\hlstd{)} \hlopt{+}
  \hlkwd{xlab}\hlstd{(}\hlkwd{TeX}\hlstd{(}\hlstr{"$\textbackslash{}\textbackslash{}mu$"}\hlstd{))} \hlopt{+}
  \hlstd{theme.info}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-29-1} 
\begin{kframe}\begin{alltt}
\hlcom{#b)}
\hlstd{posterior_mean} \hlkwb{<-} \hlstd{likelihood} \hlopt{*} \hlstd{mu}
\hlstd{posterior_mean} \hlkwb{<-} \hlkwd{sum}\hlstd{(posterior_mean)}

\hlkwd{print}\hlstd{(posterior_mean)}
\end{alltt}
\begin{verbatim}
## [1] 87.11109
\end{verbatim}
\begin{alltt}
\hlcom{#c)}
\hlstd{prob_greater_than_80} \hlkwb{<-} \hlkwd{sum}\hlstd{(likelihood[mu} \hlopt{>} \hlnum{80}\hlstd{])}

\hlkwd{print}\hlstd{(prob_greater_than_80)}
\end{alltt}
\begin{verbatim}
## [1] 0.9146066
\end{verbatim}
\end{kframe}
\end{knitrout}
\newpage
\item
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{suppressMessages}\hlstd{(}\hlkwd{library}\hlstd{(LearnBayes,} \hlkwc{quietly}\hlstd{=}\hlnum{TRUE}\hlstd{,}
                         \hlkwc{warn.conflicts} \hlstd{=} \hlnum{FALSE}\hlstd{,}\hlkwc{verbose} \hlstd{=} \hlnum{FALSE}\hlstd{))}


\hlcom{#a)}
\hlkwd{curve}\hlstd{(}\hlnum{0.5}\hlopt{*}\hlkwd{dgamma}\hlstd{(x,} \hlnum{1.5}\hlstd{,} \hlnum{1000}\hlstd{)} \hlopt{+} \hlnum{0.5}\hlopt{*}\hlkwd{dgamma}\hlstd{(x,}\hlnum{7}\hlstd{,}\hlnum{1000}\hlstd{),}
      \hlkwc{from} \hlstd{=} \hlnum{0}\hlstd{,} \hlkwc{to} \hlstd{=} \hlnum{1}\hlstd{,} \hlkwc{lwd} \hlstd{=} \hlnum{3}\hlstd{,}
      \hlkwc{xlab} \hlstd{=} \hlstr{"P"}\hlstd{,} \hlkwc{ylab} \hlstd{=} \hlstr{"Prior Density"}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-30-1} 
\begin{kframe}\begin{alltt}
\hlcom{#b)}
\hlstd{probs} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{0.5}\hlstd{,} \hlnum{0.5}\hlstd{)}
\hlstd{gamma.par1} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{1.5}\hlstd{,} \hlnum{1000}\hlstd{)}
\hlstd{gamma.par2} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{7}\hlstd{,} \hlnum{1000}\hlstd{)}
\hlstd{gammapar} \hlkwb{<-} \hlkwd{rbind}\hlstd{(gamma.par1, gamma.par2)}

\hlcom{#Data is supposed to contain a vector of counts and a vector of time intervals}
\hlcom{#How to set this up?}
\hlstd{data} \hlkwb{<-} \hlkwd{c}\hlstd{()}

\hlcom{#poisson.gamma.mix(probs, gammpar, data)}

\hlcom{#Come back to this one}
\end{alltt}
\end{kframe}
\end{knitrout}
\newpage
\item
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#a)}
\hlstd{lambda} \hlkwb{<-} \hlkwd{seq}\hlstd{(}\hlkwc{from} \hlstd{=} \hlnum{0.1}\hlstd{,} \hlkwc{to} \hlstd{=} \hlnum{1000}\hlstd{,} \hlkwc{by} \hlstd{=} \hlnum{0.1}\hlstd{)}

\hlstd{prior} \hlkwb{<-} \hlnum{1}\hlopt{/}\hlstd{lambda}

\hlstd{like} \hlkwb{<-} \hlkwd{pexp}\hlstd{(}\hlnum{100}\hlstd{,} \hlnum{1}\hlopt{/}\hlstd{lambda)}\hlopt{^}\hlnum{3} \hlopt{*} \hlkwd{dexp}\hlstd{(}\hlnum{100}\hlstd{,}\hlnum{1}\hlopt{/}\hlstd{lambda)}\hlopt{*}
  \hlstd{(}\hlkwd{pexp}\hlstd{(}\hlnum{300}\hlstd{,} \hlnum{1}\hlopt{/}\hlstd{lambda)} \hlopt{-} \hlkwd{pexp}\hlstd{(}\hlnum{100}\hlstd{,}\hlnum{1}\hlopt{/}\hlstd{lambda))}\hlopt{^}\hlnum{3}\hlopt{*}
  \hlkwd{dexp}\hlstd{(}\hlnum{300}\hlstd{,}\hlnum{1}\hlopt{/}\hlstd{lambda)}\hlopt{*}\hlstd{(}\hlnum{1}\hlopt{-}\hlkwd{pexp}\hlstd{(}\hlnum{300}\hlstd{,}\hlnum{1}\hlopt{/}\hlstd{lambda))}\hlopt{^}\hlnum{4}

\hlstd{posterior} \hlkwb{<-} \hlstd{like}\hlopt{*}\hlstd{prior}

\hlstd{posterior} \hlkwb{<-} \hlstd{posterior}\hlopt{/}\hlkwd{sum}\hlstd{(posterior)}

\hlstd{data} \hlkwb{<-} \hlkwd{data.frame}\hlstd{(}\hlkwc{lambda} \hlstd{= lambda,} \hlkwc{posterior} \hlstd{= posterior)}

\hlkwd{ggplot}\hlstd{(data,} \hlkwd{aes}\hlstd{(}\hlkwc{x}\hlstd{= lambda,} \hlkwc{y}\hlstd{= posterior))} \hlopt{+}
  \hlkwd{geom_line}\hlstd{()} \hlopt{+}
  \hlkwd{ggtitle}\hlstd{(}\hlkwd{TeX}\hlstd{(}\hlstr{"Posterior Density of $\textbackslash{}\textbackslash{}lambda$"}\hlstd{))} \hlopt{+}
  \hlkwd{ylab}\hlstd{(}\hlstr{"Posterior"}\hlstd{)} \hlopt{+}
  \hlkwd{xlab}\hlstd{(}\hlkwd{TeX}\hlstd{(}\hlstr{"$\textbackslash{}\textbackslash{}lambda$"}\hlstd{))} \hlopt{+}
  \hlstd{theme.info}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-31-1} 

\end{knitrout}

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#b)}
\hlstd{mean_lambda} \hlkwb{<-} \hlstd{lambda}\hlopt{*}\hlstd{posterior}

\hlstd{mean_lambda} \hlkwb{<-} \hlkwd{sum}\hlstd{(mean_lambda)}

\hlkwd{print}\hlstd{(mean_lambda)}
\end{alltt}
\begin{verbatim}
## [1] 327.2188
\end{verbatim}
\begin{alltt}
\hlstd{sample} \hlkwb{<-} \hlkwd{sample}\hlstd{(lambda,} \hlkwc{size} \hlstd{=} \hlnum{100000}\hlstd{,} \hlkwc{replace} \hlstd{=} \hlnum{TRUE}\hlstd{,} \hlkwc{prob} \hlstd{= posterior)}

\hlkwd{print}\hlstd{(}\hlkwd{sd}\hlstd{(sample))}
\end{alltt}
\begin{verbatim}
## [1] 127.6806
\end{verbatim}
\begin{alltt}
\hlcom{#c)}
\hlstd{prob_between_300_and_500} \hlkwb{<-} \hlkwd{sum}\hlstd{(posterior[}\hlnum{300} \hlopt{<=} \hlstd{lambda} \hlopt{&} \hlstd{lambda} \hlopt{<=} \hlnum{500}\hlstd{])}

\hlkwd{print}\hlstd{(prob_between_300_and_500)}
\end{alltt}
\begin{verbatim}
## [1] 0.4059514
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{enumerate}
\end{solution}
%\begin{thebibliography}{}
%
%\bibitem{McElreath (2016)}[rethinking]
%McElreath, R. (2016).  \emph{Statistical Rethinking: A Bayesian Course with Examples in R and Stan}.  CRC Press. 
%\end{thebibliography}
\end{document}
