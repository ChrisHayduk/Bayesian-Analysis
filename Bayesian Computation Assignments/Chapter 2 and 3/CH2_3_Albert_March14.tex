%\documentclass[12pt,addpoints]{exam}   % Print w/o solutions
\documentclass[12pt,addpoints,answers]{exam}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}   % Print solutions
%%%% comment out ONE of the above lines  
%    - the first line prints the document without the solutions, just questions
%    - the second prints the document with solutions.
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{lscape}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{multicol}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\singlespacing
%\onehalfspacing
%\doublespacing


\title{Assignment Covering Chapters 2 \& 3 of \emph{Bayesian Computation with R}}

\author{Chris Hayduk}
\date{\today}

\maketitle



\begin{questions}
\question \textbf{Chapter 2: Introduction to Bayesian Thinking} Write your own code for the following:
%%%%%%%%%
\begin{parts}
\part Simulating a sample from a posterior distribution when you have a histogram prior.  
%%%%%%%%%
\part Prior predictive density function, \texttt{pdiscp()}.  
%%%%%%%%%
\part Prior predictive density function, \texttt{pbetap()}.
%%%%%%%%%
\part Discrete credible interval, \texttt{discint}.
\end{parts}
%%%%%%%%%
\begin{solution}
\begin{parts}
\part Let's start by defining a function to generate the values for the histogram prior:
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#Function to generate prior values for each value in x}
\hlstd{get_histprior_value} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{,} \hlkwc{histprior}\hlstd{)\{}
  \hlstd{vec} \hlkwb{<-} \hlkwd{rep}\hlstd{(}\hlnum{NA}\hlstd{,} \hlkwc{length}\hlstd{=}\hlkwd{length}\hlstd{(x))}
  \hlkwa{for}\hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{length}\hlstd{(vec))\{}
    \hlstd{new_vec} \hlkwb{<-} \hlstd{histprior[histprior[,}\hlnum{1}\hlstd{]}\hlopt{>=}\hlstd{x[i],]}

    \hlcom{#Check if new_vec is matrix or vector and index accordingly}
    \hlkwa{if}\hlstd{(}\hlopt{!}\hlkwd{is.null}\hlstd{(}\hlkwd{ncol}\hlstd{(new_vec)))\{}
      \hlstd{vec[i]} \hlkwb{<-} \hlstd{new_vec[}\hlnum{1}\hlstd{,}\hlnum{2}\hlstd{]}
    \hlstd{\}} \hlkwa{else}\hlstd{\{}
      \hlstd{vec[i]} \hlkwb{<-} \hlstd{new_vec[}\hlnum{2}\hlstd{]}
    \hlstd{\}}
  \hlstd{\}}

  \hlkwd{return}\hlstd{(vec)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}
\newpage
Now let's prepare the data for input into our new \texttt{get\_histprior\_value} function:
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#Generate points for interval}
\hlstd{interval} \hlkwb{<-} \hlkwd{seq}\hlstd{(}\hlnum{0.1}\hlstd{,} \hlnum{1}\hlstd{,} \hlkwc{by} \hlstd{=} \hlnum{0.1}\hlstd{)}

\hlcom{#Prior probability}
\hlstd{prior} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{5.2}\hlstd{,} \hlnum{8}\hlstd{,} \hlnum{7.2}\hlstd{,} \hlnum{4.6}\hlstd{,} \hlnum{2.1}\hlstd{,} \hlnum{0.7}\hlstd{,} \hlnum{0.1}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{0}\hlstd{)}
\hlstd{prior} \hlkwb{<-} \hlstd{prior}\hlopt{/}\hlkwd{sum}\hlstd{(prior)}

\hlcom{#Create the histogram prior}
\hlstd{histprior} \hlkwb{<-} \hlkwd{sample}\hlstd{(interval,} \hlnum{10000}\hlstd{,} \hlkwc{replace}\hlstd{=}\hlnum{TRUE}\hlstd{,} \hlkwc{prob} \hlstd{= prior)}
\hlstd{histprior} \hlkwb{<-} \hlkwd{table}\hlstd{(histprior)}\hlopt{/}\hlkwd{sum}\hlstd{(}\hlkwd{table}\hlstd{(histprior))}
\hlstd{histprior} \hlkwb{<-} \hlkwd{as.matrix}\hlstd{(histprior)}
\hlstd{names} \hlkwb{<-} \hlkwd{rownames}\hlstd{(histprior)}
\hlkwd{rownames}\hlstd{(histprior)} \hlkwb{<-} \hlkwa{NULL}
\hlstd{histprior} \hlkwb{<-} \hlkwd{cbind}\hlstd{(}\hlkwd{as.numeric}\hlstd{(names),}\hlkwd{as.numeric}\hlstd{(histprior))}
\hlstd{histprior} \hlkwb{<-} \hlkwd{rbind}\hlstd{(histprior,} \hlkwd{c}\hlstd{(}\hlnum{0.9}\hlstd{,} \hlnum{0}\hlstd{))}
\hlstd{histprior} \hlkwb{<-} \hlkwd{rbind}\hlstd{(histprior,} \hlkwd{c}\hlstd{(}\hlnum{1.0}\hlstd{,} \hlnum{0}\hlstd{))}
\end{alltt}
\end{kframe}
\end{knitrout}
\newpage
Now let's plot the prior:
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#Output histogram of prior}
\hlkwd{curve}\hlstd{(}\hlkwd{get_histprior_value}\hlstd{(x, histprior),} \hlkwc{from}\hlstd{=}\hlnum{0}\hlstd{,} \hlkwc{to} \hlstd{=} \hlnum{1}\hlstd{,}
      \hlkwc{ylim} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{0.3}\hlstd{),} \hlkwc{n} \hlstd{=} \hlnum{10000}\hlstd{,}
      \hlkwc{xlab}\hlstd{=}\hlstr{"p"}\hlstd{,} \hlkwc{ylab} \hlstd{=} \hlstr{"Prior density"}\hlstd{,} \hlkwc{main} \hlstd{=} \hlstr{"Histogram Prior"}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-4-1} 

\end{knitrout}
\newpage
Finally, let's generate and plot the posterior:
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{s} \hlkwb{<-} \hlnum{11}
\hlstd{f} \hlkwb{<-} \hlnum{16}

\hlstd{p} \hlkwb{<-} \hlkwd{seq}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{1}\hlstd{,} \hlkwc{length} \hlstd{=} \hlnum{10000}\hlstd{)}

\hlstd{post} \hlkwb{<-} \hlkwd{get_histprior_value}\hlstd{(p, histprior)} \hlopt{*} \hlkwd{dbeta}\hlstd{(p, s}\hlopt{+}\hlnum{1}\hlstd{, f}\hlopt{+}\hlnum{1}\hlstd{)}

\hlstd{post} \hlkwb{<-} \hlstd{post}\hlopt{/}\hlkwd{sum}\hlstd{(post)}

\hlstd{ps} \hlkwb{<-} \hlkwd{sample}\hlstd{(p,} \hlkwc{replace} \hlstd{=} \hlnum{TRUE}\hlstd{,} \hlkwc{prob} \hlstd{= post)}

\hlkwd{hist}\hlstd{(ps,} \hlkwc{xlab}\hlstd{=}\hlstr{"p"}\hlstd{,} \hlkwc{main}\hlstd{=}\hlstr{"Simulated Draws from the Posterior Distribution of p"}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-5-1} 

\end{knitrout}
\newpage
\part Let's start with \texttt{pdiscp()}:
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{pdiscp} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{p}\hlstd{,} \hlkwc{prior}\hlstd{,} \hlkwc{m}\hlstd{,} \hlkwc{ys}\hlstd{)\{}
  \hlstd{pred} \hlkwb{<-} \hlkwd{rep}\hlstd{(}\hlnum{NA}\hlstd{,} \hlkwc{length} \hlstd{=} \hlkwd{length}\hlstd{(ys))}

  \hlcom{#Loop through possible y values}
  \hlkwa{for}\hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{length}\hlstd{(ys))\{}
    \hlstd{val} \hlkwb{<-} \hlnum{0}
    \hlcom{#Loop through possible proportions in discrete prior}
    \hlkwa{for}\hlstd{(j} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{length}\hlstd{(p))\{}
      \hlstd{f} \hlkwb{<-} \hlkwd{choose}\hlstd{(m, ys[i])} \hlopt{*} \hlstd{(p[j])}\hlopt{^}\hlstd{(ys[i])} \hlopt{*} \hlstd{(}\hlnum{1}\hlopt{-}\hlstd{p[j])}\hlopt{^}\hlstd{(m}\hlopt{-}\hlstd{ys[i])}
      \hlstd{g} \hlkwb{<-} \hlstd{prior[j]}
      \hlstd{val} \hlkwb{<-} \hlstd{val} \hlopt{+} \hlstd{(f}\hlopt{*}\hlstd{g)}
    \hlstd{\}}
    \hlstd{pred[i]} \hlkwb{<-} \hlstd{val}
  \hlstd{\}}

  \hlcom{#Return vector of probabilities for each y value}
  \hlkwd{return}\hlstd{(pred)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#Test run}
\hlstd{p} \hlkwb{<-} \hlkwd{seq}\hlstd{(}\hlnum{0.05}\hlstd{,} \hlnum{0.95}\hlstd{,} \hlkwc{by}\hlstd{=}\hlnum{0.1}\hlstd{)}
\hlstd{prior} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{5.2}\hlstd{,} \hlnum{8}\hlstd{,} \hlnum{7.2}\hlstd{,} \hlnum{4.6}\hlstd{,} \hlnum{2.1}\hlstd{,} \hlnum{0.7}\hlstd{,} \hlnum{0.1}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{0}\hlstd{)}
\hlstd{prior} \hlkwb{<-} \hlstd{prior}\hlopt{/}\hlkwd{sum}\hlstd{(prior)}
\hlstd{m} \hlkwb{<-} \hlnum{20}
\hlstd{ys} \hlkwb{<-} \hlnum{0}\hlopt{:}\hlnum{20}
\hlstd{pred} \hlkwb{<-} \hlkwd{pdiscp}\hlstd{(p, prior, m, ys)}

\hlkwd{round}\hlstd{(}\hlkwd{cbind}\hlstd{(}\hlnum{0}\hlopt{:}\hlnum{20}\hlstd{, pred),} \hlnum{3}\hlstd{)}
\end{alltt}
\begin{verbatim}
##           pred
##  [1,]  0 0.020
##  [2,]  1 0.044
##  [3,]  2 0.069
##  [4,]  3 0.092
##  [5,]  4 0.106
##  [6,]  5 0.112
##  [7,]  6 0.110
##  [8,]  7 0.102
##  [9,]  8 0.089
## [10,]  9 0.074
## [11,] 10 0.059
## [12,] 11 0.044
## [13,] 12 0.031
## [14,] 13 0.021
## [15,] 14 0.013
## [16,] 15 0.007
## [17,] 16 0.004
## [18,] 17 0.002
## [19,] 18 0.001
## [20,] 19 0.000
## [21,] 20 0.000
\end{verbatim}
\end{kframe}
\end{knitrout}
\newpage
\part Now let's try \texttt{pbetap()}:
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{pbetap} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{ab}\hlstd{,} \hlkwc{m}\hlstd{,} \hlkwc{ys}\hlstd{)\{}
  \hlstd{pred} \hlkwb{<-} \hlkwd{rep}\hlstd{(}\hlnum{NA}\hlstd{,} \hlkwc{length} \hlstd{=} \hlkwd{length}\hlstd{(ys))}

  \hlkwa{for}\hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{length}\hlstd{(ys))\{}
    \hlstd{choose} \hlkwb{<-} \hlkwd{choose}\hlstd{(m, ys[i])}

    \hlstd{beta_num} \hlkwb{<-} \hlkwd{beta}\hlstd{(ab[}\hlnum{1}\hlstd{]} \hlopt{+} \hlstd{ys[i], ab[}\hlnum{2}\hlstd{]} \hlopt{+} \hlstd{m} \hlopt{-} \hlstd{ys[i])}

    \hlstd{beta_denom} \hlkwb{<-} \hlkwd{beta}\hlstd{(ab[}\hlnum{1}\hlstd{], ab[}\hlnum{2}\hlstd{])}

    \hlstd{pred[i]} \hlkwb{<-} \hlstd{choose}\hlopt{*}\hlstd{(beta_num}\hlopt{/}\hlstd{beta_denom)}
  \hlstd{\}}

  \hlcom{#Return vector of probabilities for each y value}
  \hlkwd{return}\hlstd{(pred)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#Test run}
\hlstd{ab} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{3.26}\hlstd{,} \hlnum{7.19}\hlstd{)}
\hlstd{m} \hlkwb{<-} \hlnum{20}
\hlstd{ys} \hlkwb{<-} \hlnum{0}\hlopt{:}\hlnum{20}
\hlstd{pred} \hlkwb{<-} \hlkwd{pbetap}\hlstd{(ab, m, ys)}
\hlkwd{round}\hlstd{(}\hlkwd{cbind}\hlstd{(}\hlnum{0}\hlopt{:}\hlnum{20}\hlstd{, pred),} \hlnum{3}\hlstd{)}
\end{alltt}
\begin{verbatim}
##           pred
##  [1,]  0 0.018
##  [2,]  1 0.045
##  [3,]  2 0.072
##  [4,]  3 0.095
##  [5,]  4 0.108
##  [6,]  5 0.114
##  [7,]  6 0.111
##  [8,]  7 0.102
##  [9,]  8 0.088
## [10,]  9 0.073
## [11,] 10 0.057
## [12,] 11 0.043
## [13,] 12 0.030
## [14,] 13 0.020
## [15,] 14 0.012
## [16,] 15 0.007
## [17,] 16 0.004
## [18,] 17 0.002
## [19,] 18 0.001
## [20,] 19 0.000
## [21,] 20 0.000
\end{verbatim}
\end{kframe}
\end{knitrout}
\newpage
\part Finally, here's \texttt{discint}:
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{discint} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{dist}\hlstd{,} \hlkwc{covprob}\hlstd{)\{}
  \hlstd{total_prob} \hlkwb{<-} \hlnum{0}
  \hlstd{y_list} \hlkwb{<-} \hlkwd{c}\hlstd{()}

  \hlcom{#Find max probability value}
  \hlcom{#Centers credible interval around y value with max probability}
  \hlstd{i} \hlkwb{<-} \hlkwd{which.is.max}\hlstd{(dist[,}\hlnum{2}\hlstd{])}
  \hlstd{total_prob} \hlkwb{<-} \hlstd{total_prob} \hlopt{+} \hlstd{dist[i,}\hlnum{2}\hlstd{]}
  \hlstd{y_list} \hlkwb{<-} \hlkwd{c}\hlstd{(y_list, dist[i,}\hlnum{1}\hlstd{])}

  \hlstd{i_prev} \hlkwb{<-} \hlstd{i}\hlopt{-}\hlnum{1}
  \hlstd{i_next} \hlkwb{<-} \hlstd{i}\hlopt{+}\hlnum{1}

  \hlkwa{while}\hlstd{(total_prob} \hlopt{<} \hlstd{covprob)\{}
    \hlcom{#Check if previous row is within range of matrix}
    \hlkwa{if}\hlstd{(i_prev} \hlopt{>=} \hlnum{1} \hlopt{&} \hlstd{i_prev} \hlopt{<=} \hlkwd{nrow}\hlstd{(dist))\{}
      \hlstd{prev_val} \hlkwb{<-} \hlstd{dist[i_prev,}\hlnum{2}\hlstd{]}
    \hlstd{\}} \hlkwa{else}\hlstd{\{}
      \hlstd{prev_val} \hlkwb{<-} \hlopt{-}\hlnum{1}
    \hlstd{\}}

    \hlcom{#Check if next row is within range of matrix}
    \hlkwa{if}\hlstd{(i_next} \hlopt{>=} \hlnum{1} \hlopt{&} \hlstd{i_next} \hlopt{<=} \hlkwd{nrow}\hlstd{(dist))\{}
      \hlstd{next_val} \hlkwb{<-} \hlstd{dist[i_next,}\hlnum{2}\hlstd{]}
    \hlstd{\}} \hlkwa{else}\hlstd{\{}
      \hlstd{next_val} \hlkwb{<-} \hlopt{-}\hlnum{1}
    \hlstd{\}}

    \hlcom{#Compare values for i_next and i_prev}
    \hlkwa{if}\hlstd{(dist[i_next,}\hlnum{2}\hlstd{]} \hlopt{>=} \hlstd{dist[i_prev,}\hlnum{2}\hlstd{])\{}
      \hlstd{total_prob} \hlkwb{<-} \hlstd{total_prob} \hlopt{+} \hlstd{dist[i_next,} \hlnum{2}\hlstd{]}
      \hlstd{y_list} \hlkwb{<-} \hlkwd{c}\hlstd{(y_list, dist[i_next,} \hlnum{1}\hlstd{])}
      \hlstd{i_next} \hlkwb{<-} \hlstd{i_next} \hlopt{+} \hlnum{1}
    \hlstd{\}} \hlkwa{else}\hlstd{\{}
      \hlstd{total_prob} \hlkwb{<-} \hlstd{total_prob} \hlopt{+} \hlstd{dist[i_prev,} \hlnum{2}\hlstd{]}
      \hlstd{y_list} \hlkwb{<-} \hlkwd{c}\hlstd{(y_list, dist[i_prev,} \hlnum{1}\hlstd{])}
      \hlstd{i_prev} \hlkwb{<-} \hlstd{i_prev} \hlopt{-} \hlnum{1}
    \hlstd{\}}
  \hlstd{\}}

  \hlstd{output} \hlkwb{<-} \hlkwd{list}\hlstd{(}\hlkwc{prob} \hlstd{= total_prob,} \hlkwc{set} \hlstd{=} \hlkwd{sort}\hlstd{(y_list))}

  \hlkwd{return}\hlstd{(output)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#Test run}
\hlstd{p} \hlkwb{<-} \hlkwd{rbeta}\hlstd{(}\hlnum{1000}\hlstd{,} \hlnum{3.26}\hlstd{,} \hlnum{7.19}\hlstd{)}
\hlstd{y} \hlkwb{<-} \hlkwd{rbinom}\hlstd{(}\hlnum{1000}\hlstd{,} \hlnum{20}\hlstd{, p)}
\hlstd{freq} \hlkwb{<-} \hlkwd{table}\hlstd{(y)}
\hlstd{ys} \hlkwb{<-} \hlkwd{as.integer}\hlstd{(}\hlkwd{names}\hlstd{(freq))}
\hlstd{predprob} \hlkwb{<-} \hlstd{freq}\hlopt{/}\hlkwd{sum}\hlstd{(freq)}

\hlstd{dist} \hlkwb{<-} \hlkwd{cbind}\hlstd{(ys, predprob)}
\hlstd{covprob} \hlkwb{<-} \hlnum{0.9}

\hlkwd{discint}\hlstd{(dist, covprob)}
\end{alltt}
\begin{verbatim}
## $prob
## [1] 0.902
## 
## $set
##  [1]  1  2  3  4  5  6  7  8  9 10 11
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{parts}
\end{solution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\question \textbf{Chapter 2: Introduction to Bayesian Thinking}  Show how the author derived the predictive density formula below with a $beta(a,b)$ prior (pg. 31 of book):
\begin{eqnarray*}
f(\tilde{y}) &=& \int f_B(\tilde{y}|m,p)g(p)dp \\
             &=& {m\choose\tilde{y}}\frac{B(a+\tilde{y},b+m-\tilde{y})}{B(a,b)}
\end{eqnarray*}
where $\tilde{y}=0,\ldots,m$.
%%%%%%%%%
\begin{solution}
\begin{align*}
f(\tilde{y}) &= \int f_B(\tilde{y}|m,p)g(p)dp \\
&= \int_{0}^{1} \left({m\choose \tilde{y}}p^{\tilde{y}}(1-p)^{m-\tilde{y}}\left(\int_{0}^{1} p^{a-1}(1-p)^{b-1} dp\right)\right) dp\\
&= {m\choose \tilde{y}} \int_{0}^{1} \left(p^{\tilde{y}}(1-p)^{m-\tilde{y}}\left(\int_{0}^{1} p^{a-1}(1-p)^{b-1} dp\right)\right) dp\\
&= {m\choose \tilde{y}} \int_{0}^{1} \left(p^{\tilde{y}}(1-p)^{m-\tilde{y}}\left(B(a, b)\right)\right) dp\\
&= {m\choose \tilde{y}} B(a,b) \int_{0}^{1} p^{\tilde{y}}(1-p)^{m-\tilde{y}} dp\\
&= {m\choose \tilde{y}} B(a,b) B(\tilde{y} + 1, m - \tilde{y} + 1)
\end{align*}
Not sure where to go from here. \textbf{Come back to this one later.}
\end{solution}
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\question \textbf{Chapter 2: Introduction to Bayesian Thinking} Exercise 5.
%%%%%%%%%
\begin{solution}
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{mu} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{20}\hlstd{,} \hlnum{30}\hlstd{,} \hlnum{40}\hlstd{,} \hlnum{50}\hlstd{,} \hlnum{60}\hlstd{,} \hlnum{70}\hlstd{)}
\hlstd{prior} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{.1}\hlstd{,} \hlnum{.15}\hlstd{,} \hlnum{.25}\hlstd{,} \hlnum{.25}\hlstd{,} \hlnum{.15}\hlstd{,} \hlnum{.1}\hlstd{)}
\hlstd{prior} \hlkwb{<-} \hlstd{prior}\hlopt{/}\hlkwd{sum}\hlstd{(prior)}

\hlstd{y} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{38.6}\hlstd{,} \hlnum{42.4}\hlstd{,} \hlnum{57.5}\hlstd{,} \hlnum{40.5}\hlstd{,} \hlnum{51.7}\hlstd{,} \hlnum{67.1}\hlstd{,} \hlnum{33.4}\hlstd{,} \hlnum{60.9}\hlstd{,} \hlnum{64.1}\hlstd{,} \hlnum{40.1}\hlstd{,} \hlnum{40.7}\hlstd{,} \hlnum{6.4}\hlstd{)}
\hlstd{ybar} \hlkwb{<-} \hlkwd{mean}\hlstd{(y)}

\hlstd{n} \hlkwb{<-} \hlkwd{length}\hlstd{(y)}
\hlstd{sigma_squared} \hlkwb{<-} \hlkwd{var}\hlstd{(y)}

\hlstd{like} \hlkwb{<-} \hlkwd{exp}\hlstd{(}\hlopt{-}\hlstd{(n}\hlopt{/}\hlstd{(}\hlnum{2}\hlopt{*}\hlstd{sigma_squared))}\hlopt{*}\hlstd{(mu} \hlopt{-} \hlstd{ybar)}\hlopt{^}\hlnum{2}\hlstd{)}
\hlstd{post} \hlkwb{<-} \hlstd{prior}\hlopt{*}\hlstd{like}\hlopt{/}\hlkwd{sum}\hlstd{(prior}\hlopt{*}\hlstd{like)}

\hlstd{dist} \hlkwb{<-} \hlkwd{cbind}\hlstd{(mu, post)}
\hlkwd{discint}\hlstd{(dist,} \hlnum{0.8}\hlstd{)}
\end{alltt}
\begin{verbatim}
## $prob
##      post 
## 0.9921244 
## 
## $set
## mu mu 
## 40 50
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{solution}
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\question \textbf{Chapter 2: Introduction to Bayesian Thinking} Exercise 6.
%%%%%%%%%
\begin{solution}
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#a)}
\hlstd{lambda} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{0.5}\hlstd{,} \hlnum{1}\hlstd{,} \hlnum{1.5}\hlstd{,} \hlnum{2}\hlstd{,} \hlnum{2.5}\hlstd{,} \hlnum{3}\hlstd{)}
\hlstd{prior} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{0.1}\hlstd{,} \hlnum{0.2}\hlstd{,} \hlnum{0.3}\hlstd{,} \hlnum{0.2}\hlstd{,} \hlnum{0.15}\hlstd{,} \hlnum{0.05}\hlstd{)}
\hlstd{prior} \hlkwb{<-} \hlstd{prior}\hlopt{/}\hlkwd{sum}\hlstd{(prior)}

\hlstd{pred} \hlkwb{<-} \hlkwd{rep}\hlstd{(}\hlnum{NA}\hlstd{,} \hlkwc{length} \hlstd{=} \hlkwd{length}\hlstd{(lambda))}
\hlstd{y} \hlkwb{<-} \hlnum{12}

\hlkwa{for}\hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{length}\hlstd{(lambda))\{}
  \hlstd{pred[i]} \hlkwb{<-} \hlstd{prior[i]}\hlopt{*}\hlkwd{exp}\hlstd{(}\hlopt{-}\hlnum{6}\hlopt{*}\hlstd{lambda[i])}\hlopt{*}\hlstd{(}\hlnum{6}\hlopt{*}\hlstd{lambda[i])}\hlopt{^}\hlnum{12}
\hlstd{\}}
\hlstd{pred} \hlkwb{<-} \hlstd{pred}\hlopt{/}\hlkwd{sum}\hlstd{(pred)}
\hlstd{pred} \hlkwb{<-} \hlkwd{cbind}\hlstd{(lambda, pred)}
\hlkwd{print}\hlstd{(}\hlkwd{round}\hlstd{(pred,}\hlnum{5}\hlstd{))}
\end{alltt}
\begin{verbatim}
##      lambda    pred
## [1,]    0.5 0.00009
## [2,]    1.0 0.03679
## [3,]    1.5 0.35652
## [4,]    2.0 0.37357
## [5,]    2.5 0.20299
## [6,]    3.0 0.03004
\end{verbatim}
\begin{alltt}
\hlcom{#b)}
\hlstd{predictive_prob} \hlkwb{<-} \hlkwd{rep}\hlstd{(}\hlnum{NA}\hlstd{,} \hlkwc{length} \hlstd{=} \hlkwd{length}\hlstd{(lambda))}
\hlkwa{for}\hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{length}\hlstd{(lambda))\{}
  \hlstd{predictive_prob[i]} \hlkwb{<-} \hlkwd{exp}\hlstd{(}\hlopt{-}\hlnum{7}\hlopt{*}\hlstd{lambda[i])}\hlopt{*}\hlstd{pred[i]}
\hlstd{\}}

\hlkwd{sum}\hlstd{(predictive_prob)}
\end{alltt}
\begin{verbatim}
## [1] 0.01605361
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{solution}
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\question  \textbf{Chapter 3: Single-Parameter Models} Write your own versions of the functions \texttt{beta.binomial.mix()} and \texttt{pbetat()}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\question \textbf{Chapter 3: Single-Parameter Models} On pg. 55, Albert writes, ``One can show that the posterior probability that the coin is fair is given by:''
\begin{eqnarray*}
\lambda(y) &=& \frac{0.5P_0(Y\leq 5)}{0.5P_0(Y\leq 5) + 0.5P_1(Y\leq 5)}.
\end{eqnarray*}
Show this.  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\question \textbf{Chapter 3: Single-Parameter Models} Do all Exercises at the end of the chapter.  
\end{questions}


%\begin{thebibliography}{}
%
%\bibitem{McElreath (2016)}[rethinking]
%McElreath, R. (2016).  \emph{Statistical Rethinking: A Bayesian Course with Examples in R and Stan}.  CRC Press. 
%\end{thebibliography}

\end{document}
